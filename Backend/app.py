from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from semantic_search import hybrid_search
import google.generativeai as genai

# ğŸ” Hardcoded API key for testing â€” REMOVE in production!
genai.configure(api_key="AIzaSyBeQUdqY2x_kuhO-l3zYB--gH5lOOIYwAo")
model = genai.GenerativeModel("models/gemini-1.5-flash-latest")

# --- FastAPI Setup ---
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Request Schema ---
class SearchRequest(BaseModel):
    query: str
    top_k: int = 50
    alpha: float = 0.6

# --- Step 1: Paraphrase the User's Question ---
def generate_paraphrased_questions(question: str, n: int = 15) -> list[str]:
    prompt = f"""
Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ {n} Ù…Ø±Ø§Øª Ø¨ØµÙŠØº Ù…Ø®ØªÙ„ÙØ© ÙˆÙ„ÙƒÙ† Ø¨Ù†ÙØ³ Ø§Ù„Ù…Ø¹Ù†Ù‰ØŒ Ù„Ø§ ØªØ¶Ù Ø¥Ø¬Ø§Ø¨Ø©ØŒ ÙÙ‚Ø· Ø§Ù„ØµÙŠØ§ØºØ§Øª:

Ø§Ù„Ø³Ø¤Ø§Ù„: {question}

Ø§Ù„ØµÙŠØ§ØºØ§Øª:
"""
    response = model.generate_content(prompt)
    return [line.strip("- ").strip() for line in response.text.strip().split("\n") if line.strip()]

# --- Step 2: Search Using All Reformulations ---
def multi_query_search(queries: list[str], top_k: int, alpha: float) -> list[dict]:
    seen = set()
    combined = []
    for q in queries:
        results = hybrid_search(q, top_k=top_k, alpha=alpha)
        for r in results:
            uid = r.get("id") or f"{r['article_number']}-{r['text'][:30]}"
            if uid not in seen:
                combined.append(r)
                seen.add(uid)
    return combined

# --- Step 3: Gemini Prompt + Clarification Fallback ---
@app.post("/search")
async def search(req: SearchRequest):
    # Reformulate the query
    paraphrased = generate_paraphrased_questions(req.query)
    paraphrased.insert(0, req.query)  # include original
    results = multi_query_search(paraphrased, top_k=req.top_k, alpha=req.alpha)

    # Create Gemini prompt
    context = "\n\n".join([f"Ø§Ù„Ù…Ø§Ø¯Ø© {r['article_number']}:\n{r['text']}" for r in results])
    prompt = f"""
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø°ÙƒÙŠ. Ø¥Ù„ÙŠÙƒ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ù…ÙˆØ§Ø¯ Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª Ø§Ù„Ù„Ø¨Ù†Ø§Ù†ÙŠ.

- Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø§Ø¯Ø© **Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø© ÙÙ‚Ø·** Ù…Ù† Ø¨ÙŠÙ† Ø§Ù„Ù…ÙˆØ§Ø¯.
- Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø£ÙŠ Ù…Ø§Ø¯Ø© Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ Ø£Ø®Ø¨Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ù† Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© ØºÙŠØ± ÙƒØ§ÙÙŠØ© ÙˆØ£Ø·Ù„Ø¨ Ù…Ù†Ù‡ ØªÙˆØ¶ÙŠØ­ Ø³Ø¤Ø§Ù„Ù‡.

Ø§Ù„Ù…ÙˆØ§Ø¯:
{context}

Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
{req.query}

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
"""
    # Get answer
    response = model.generate_content(prompt)
    answer = response.text.strip()

    # Optional: check for ambiguous replies manually
    ambiguous_phrases = [
        "Ù„Ø§ ØªÙˆØ¬Ø¯ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø¯", "Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©", "ØºÙŠØ± ÙƒØ§ÙÙŠØ©", "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø§Ø¯Ø©"
    ]
    if any(p in answer for p in ambiguous_phrases):
        answer += "\n\nğŸ” Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªÙˆØ¶ÙŠØ­ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙƒØ«Ø± Ø­ØªÙ‰ Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ø¯Ù‚Ø©."

    return {"answer": answer, "sources": results}
