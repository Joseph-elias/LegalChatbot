from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from semantic_search import semantic_search_only  # changed import!
import google.generativeai as genai

# ğŸ” Hardcoded API key for testing â€” REMOVE in production!
genai.configure(api_key="AIzaSyBeQUdqY2x_kuhO-l3zYB--gH5lOOIYwAo")
model = genai.GenerativeModel("models/gemini-1.5-flash-latest")

# --- FastAPI Setup ---
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Request Schema ---
class SearchRequest(BaseModel):
    query: str
    top_k: int = 150

# --- Step 1: Paraphrase the User's Question ---
def generate_paraphrased_questions(question: str, n: int = 15) -> list[str]:
    prompt = f"""
Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ {n} Ù…Ø±Ø§Øª Ø¨ØµÙŠØº Ù…Ø®ØªÙ„ÙØ© ÙˆÙ„ÙƒÙ† Ø¨Ù†ÙØ³ Ø§Ù„Ù…Ø¹Ù†Ù‰ØŒ Ù„Ø§ ØªØ¶Ù Ø¥Ø¬Ø§Ø¨Ø©ØŒ ÙÙ‚Ø· Ø§Ù„ØµÙŠØ§ØºØ§Øª:

Ø§Ù„Ø³Ø¤Ø§Ù„: {question}

Ø§Ù„ØµÙŠØ§ØºØ§Øª:
"""
    response = model.generate_content(
    prompt,
    generation_config={"temperature": 0.3}
)
    return [line.strip("- ").strip() for line in response.text.strip().split("\n") if line.strip()]

# --- Step 2: Search Using All Reformulations ---
def multi_query_search(queries: list[str], top_k: int) -> list[dict]:
    seen = set()
    combined = []
    for q in queries:
        results = semantic_search_only(q, top_k=top_k)
        for r in results:
            uid = r["doc_id"]
            if uid not in seen:
                combined.append(r)
                seen.add(uid)
    return combined

# --- Step 3: Gemini Reranking ---
def rerank_with_llm(results, user_query):
    context = "\n\n".join([f"Ø§Ù„Ù…Ø§Ø¯Ø© {r['article_number']}:\n{r['text']}" for r in results])
    prompt = f"""
Ø§Ø®ØªØ± Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„ÙˆØ­ÙŠØ¯Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø© Ù…Ù† Ø¨ÙŠÙ† Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ØªØ§Ù„ÙŠØ© Ø§Ù„ØªÙŠ ØªØ¬ÙŠØ¨ Ø¹Ù† Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ Ù„ÙƒÙ† Ø§Ù†ØªØ¨Ù‡:
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØªØ­Ø¯Ø« Ø¹Ù† "Ø¬Ù†Ø§ÙŠØ©" ÙŠØ¬Ø¨ Ø£Ù† ØªØ®ØªØ§Ø± ÙÙ‚Ø· Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ ÙƒÙ„Ù…Ø© "Ø¬Ù†Ø§ÙŠØ©" ÙˆÙ„ÙŠØ³ "Ø¬Ù†Ø­Ø©".
- ÙˆØ¥Ø°Ø§ ÙƒØ§Ù† ÙŠØªØ­Ø¯Ø« Ø¹Ù† "Ø¬Ù†Ø­Ø©" Ø§Ø®ØªØ± ÙÙ‚Ø· Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ ÙƒÙ„Ù…Ø© "Ø¬Ù†Ø­Ø©".
Ø«Ù…:
- Ø§Ø°ÙƒØ± Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø©ØŒ
- Ø§Ù†Ø³Ø® Ù†Øµ Ø§Ù„Ù…Ø§Ø¯Ø© ÙƒØ§Ù…Ù„Ø©ØŒ
- ÙˆØ§Ø´Ø±Ø­ Ø¨Ø§Ø®ØªØµØ§Ø± Ø³Ø¨Ø¨ Ø§Ø®ØªÙŠØ§Ø±Ùƒ.
Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ù…Ø§Ø¯Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ø¨Ø¯Ù‚Ø©ØŒ Ø£Ø®Ø¨Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø°Ù„Ùƒ ÙˆØ§Ø·Ù„Ø¨ Ù…Ù†Ù‡ ØªÙˆØ¶ÙŠØ­ Ø³Ø¤Ø§Ù„Ù‡.


Ø§Ù„Ù…ÙˆØ§Ø¯:
{context}

Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
{user_query}

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
"""
    response = model.generate_content(prompt)
    return response.text.strip()

# --- Search Endpoint ---
@app.post("/search")
async def search(req: SearchRequest):
    # Step 1: Reformulate the query
    paraphrased = generate_paraphrased_questions(req.query)
    paraphrased.insert(0, req.query)  # include original

    # Step 2: Dense search for all paraphrases, deduplicated
    results = multi_query_search(paraphrased, top_k=req.top_k)
    results = sorted(results, key=lambda x: -x["score"])[:req.top_k]

    # Step 3: LLM reranking prompt (Gemini picks and explains)
    answer = rerank_with_llm(results, req.query)

    # Optional: clarification if ambiguous
    ambiguous_phrases = [
        "Ù„Ø§ ØªÙˆØ¬Ø¯ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø¯", "Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©", "ØºÙŠØ± ÙƒØ§ÙÙŠØ©", "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø§Ø¯Ø©"
    ]
    if any(p in answer for p in ambiguous_phrases):
        answer += "\n\nğŸ” Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªÙˆØ¶ÙŠØ­ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙƒØ«Ø± Ø­ØªÙ‰ Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ø¯Ù‚Ø©."

    return {"answer": answer, "sources": results}
