from fastapi import FastAPI
from pydantic import BaseModel
from semantic_search import embedder ,normalize_arabic_text,load_embeddings # ,semantic_search_only # changed import!
import google.generativeai as genai
import json
import os
import torch
import numpy as np
from sentence_transformers import util, SentenceTransformer
import google.generativeai as genai
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import pyarabic.araby as araby
import os
from dotenv import load_dotenv

# --- FastAPI Setup ---
app = FastAPI()

origins = [
    "https://leblegalchatbot.onrender.com",  
]




app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # TEMPORARY for testing â€” wide open
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)





load_dotenv()  

genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
model = genai.GenerativeModel("models/gemini-1.5-flash-latest")


# â”€â”€ F) Dense-only semantic search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def semantic_search_only(query: str, top_k: int = 5):
    global corpus_ids, corpus_texts, corpus_emb
    query = normalize_arabic_text(query)
    # This function will use the globally defined 'embedder' and 'corpus_emb'
   
    q_emb = embedder.encode([query], convert_to_tensor=True, normalize_embeddings=True)
    # Ensure corpus_emb is loaded and not empty, handle defensively for worker execution
    if 'corpus_emb' not in globals() or not hasattr(corpus_emb, 'nelement') or corpus_emb.nelement() == 0:
        print("Error: corpus_emb not loaded or empty in semantic_search_only.")
        # Attempt to load corpus_emb if it's missing, for robustness during __main__ or direct calls
        corpus_ids, corpus_texts, corpus_emb = load_embeddings()
        if not hasattr(corpus_emb, 'nelement') or corpus_emb.nelement() == 0:
             print("Error: Failed to load corpus_emb, it remains empty.")
             return []


    cos_scores = util.cos_sim(q_emb, corpus_emb)[0].cpu().numpy()
    top_idxs = np.argsort(cos_scores)[::-1][:top_k]

    results = []
    # Ensure corpus_ids is also loaded and available
    if 'corpus_ids' not in globals():
        print("Error: corpus_ids not loaded in semantic_search_only.")
        return [] # Or handle error appropriately

    for i in top_idxs:
        # Defensive check for corpus_ids length
        if i < len(corpus_ids):
            num = corpus_ids[i].split("_")[-1]
            results.append({
                "article_number": num,
                "doc_id": corpus_ids[i],
                "score": float(cos_scores[i]),
                "text": corpus_texts[i] 
            })
        else:
            print(f"Warning: Index {i} out of bounds for corpus_ids.")
    return results



# --- Request Schema ---
class SearchRequest(BaseModel):
    query: str
    top_k: int = 150

# --- Step 1: Paraphrase the User's Question ---
def generate_paraphrased_questions(question: str, n: int = 54) -> list[str]:
    prompt = f"""
Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ {n} Ù…Ø±Ø© Ø¨ØµÙŠØ§ØºØ§Øª Ù…Ø®ØªÙ„ÙØ© ÙˆÙ„ÙƒÙ† Ø¨Ù†ÙØ³ Ø§Ù„Ù…Ø¹Ù†Ù‰ØŒ Ù…Ø³ØªØ®Ø¯Ù…Ù‹Ø§ Ø£Ø³Ù„ÙˆØ¨Ù‹Ø§ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠÙ‹Ø§ ÙˆØ±Ø³Ù…ÙŠÙ‹Ø§ ÙƒÙ…Ø§ Ù„Ùˆ Ø£Ù†Ùƒ ØªÙƒØªØ¨ Ù„Ù…Ù‚Ø§Ù„ Ø¹Ù„Ù…ÙŠ Ø£Ùˆ ØªÙ‚Ø±ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ. Ù„Ø§ ØªØ¶Ù Ø¥Ø¬Ø§Ø¨Ø©ØŒ ÙÙ‚Ø· Ø§Ù„ØµÙŠØ§ØºØ§Øª.

Ø§Ù„Ø³Ø¤Ø§Ù„: {question}

Ø§Ù„ØµÙŠØ§ØºØ§Øª:
"""
    response = model.generate_content(
        prompt,
        generation_config={"temperature": 0.0}
    )
    return [line.strip("- ").strip() for line in response.text.strip().splitlines() if line.strip()]


# --- Step 2: Search Using All Reformulations ---
def multi_query_search(queries: list[str], top_k: int) -> list[dict]:
    seen = set()
    combined = []
    for q in queries:
        results = semantic_search_only(q, top_k=top_k)
        for r in results:
            uid = r["doc_id"]
            if uid not in seen:
                combined.append(r)
                seen.add(uid)
    return combined

# --- Step 3: Gemini Reranking ---
def rerank_with_llm(results, user_query):
    context = "".join([f"Ø§Ù„Ù…Ø§Ø¯Ø© {r['article_number']}:{r['text']}" for r in results])
    prompt = f"""
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© **Ø§Ù„ÙˆØ­ÙŠØ¯Ø©** Ø§Ù„Ø£ÙƒØ«Ø± ØªØ·Ø§Ø¨Ù‚Ù‹Ø§ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ù† Ø¶Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø¹Ø·Ø§Ø© Ù„Ùƒ ÙÙ‚Ø·. Ø§ØªØ¨Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø¯Ù‚Ø©:

1.  **ØªØ­Ù„ÙŠÙ„ Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ù„Ù…ÙˆØ§Ø¯:** Ø§Ù‚Ø±Ø£ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø¹Ù†Ø§ÙŠØ© ÙØ§Ø¦Ù‚Ø©.
2.  **Ø´Ø±Ø· "Ø¬Ù†Ø§ÙŠØ©" Ùˆ "Ø¬Ù†Ø­Ø©":**
    *   Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØªØ¹Ù„Ù‚ Ø¨Ù€ "Ø¬Ù†Ø§ÙŠØ©" Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ®ØªØ§Ø± Ù…Ø§Ø¯Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø© "Ø¬Ù†Ø§ÙŠØ©" Ø£Ùˆ ØªØªØ¹Ù„Ù‚ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± Ø¨Ø§Ù„Ø¬Ù†Ø§ÙŠØ§Øª. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ØŒ Ø§Ø°ÙƒØ± Ø°Ù„Ùƒ.
    *   Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØªØ¹Ù„Ù‚ Ø¨Ù€ "Ø¬Ù†Ø­Ø©" Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ®ØªØ§Ø± Ù…Ø§Ø¯Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø© "Ø¬Ù†Ø­Ø©" Ø£Ùˆ ØªØªØ¹Ù„Ù‚ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± Ø¨Ø§Ù„Ø¬Ù†Ø­. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ØŒ Ø§Ø°ÙƒØ± Ø°Ù„Ùƒ.
    *   Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ø¶Ø­Ù‹Ø§ Ø¨Ø´Ø£Ù† "Ø¬Ù†Ø§ÙŠØ©" Ø£Ùˆ "Ø¬Ù†Ø­Ø©"ØŒ Ø£Ùˆ Ù„Ù… ÙŠÙ†Ø·Ø¨Ù‚ Ø§Ù„Ø´Ø±Ø·ØŒ Ø§Ø®ØªØ± Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø© Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù….
3.  **Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± ÙˆØ§Ù„Ø¥Ø®Ø±Ø§Ø¬:**
    *   Ø§Ø®ØªØ± Ø§Ù„Ù…Ø§Ø¯Ø© **Ø§Ù„ÙˆØ­ÙŠØ¯Ø©** Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø©.
    *   **ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚ØªØµØ± Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¶Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø¹Ø·Ø§Ø© ÙÙ‚Ø·. Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ©.**
    *   Ø§Ø°ÙƒØ± Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„ØªÙŠ Ø§Ø®ØªØ±ØªÙ‡Ø§.
    *   Ø§Ù†Ø³Ø® Ù†Øµ Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„ØªÙŠ Ø§Ø®ØªØ±ØªÙ‡Ø§ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ ÙƒÙ…Ø§ Ù‡Ùˆ.
    *   Ø§Ø´Ø±Ø­ Ø¨Ø¥ÙŠØ¬Ø§Ø² Ø³Ø¨Ø¨ Ø§Ø®ØªÙŠØ§Ø±Ùƒ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø§Ø¯Ø© ÙˆÙƒÙŠÙ ØªØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ Ù…Ø¹ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ù…Ù† Ø§Ù„Ù…Ø§Ø¯Ø©.
4.  **ÙÙŠ Ø­Ø§Ù„Ø© Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù…Ø§Ø¯Ø© Ù…Ù†Ø§Ø³Ø¨Ø©:** Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø£ÙŠ Ù…Ø§Ø¯Ø© Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø¹Ø·Ø§Ø© ØªØ¬ÙŠØ¨ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø§Ø³Ø¨ ÙˆØ¯Ù‚ÙŠÙ‚ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ Ø­ØªÙ‰ Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø´Ø±ÙˆØ· "Ø¬Ù†Ø§ÙŠØ©" Ùˆ "Ø¬Ù†Ø­Ø©"ØŒ Ø§Ø°ÙƒØ± Ø¨ÙˆØ¶ÙˆØ­: "Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø§Ø¯Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø¹Ø·Ø§Ø© ØªØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø¯Ù‚Ø©. ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø£Ùˆ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨." Ù„Ø§ ØªØ­Ø§ÙˆÙ„ Ø§Ø®ØªÙŠØ§Ø± Ù…Ø§Ø¯Ø© ØºÙŠØ± Ù…Ù„Ø§Ø¦Ù…Ø©.

Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:
{context}

Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
{user_query}

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø®ØªØ§Ø±Ø© (Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø©ØŒ Ù†Øµ Ø§Ù„Ù…Ø§Ø¯Ø©ØŒ Ø´Ø±Ø­ Ø§Ù„Ø³Ø¨Ø¨):
"""
    response = model.generate_content(
        prompt,
        generation_config={"temperature": 0.0}
    )
    return response.text.strip()

# --- Search Endpoint ---
@app.post("/search")
async def search(req: SearchRequest):
    # Ensure corpus_ids, corpus_texts, and corpus_emb are loaded globally.
    # This would typically be done once at startup by FastAPI's lifespan events,
    # or as done in __main__. For simplicity here, ensure they are loaded.
    global corpus_ids, corpus_texts, corpus_emb
    if 'corpus_ids' not in globals() or \
       'corpus_texts' not in globals() or \
       'corpus_emb' not in globals() or \
       not hasattr(corpus_emb, 'nelement') or \
       corpus_emb.nelement() == 0:
        print("Corpus not loaded or empty in /search endpoint. Attempting to load.")
        # This call is critical for the FastAPI app if not loaded at startup
        corpus_ids, corpus_texts, corpus_emb = load_embeddings()
        if not hasattr(corpus_emb, 'nelement') or corpus_emb.nelement() == 0:
            print("Error: Failed to load corpus_emb for /search, it remains empty.")
            # Depending on requirements, might return an error response to client
            # For now, processing will likely fail in multi_query_search if emb is empty

    normalized_query = normalize_arabic_text(req.query)

    # Step 1: Reformulate the query
    paraphrased = generate_paraphrased_questions(normalized_query)
    paraphrased.insert(0, normalized_query)  # include original

    # Step 2: Dense search for all paraphrases, deduplicated
    results = multi_query_search(paraphrased, top_k=req.top_k) # This calls semantic_search_only
    results = sorted(results, key=lambda x: -x["score"])[:req.top_k]

    # Step 3: LLM reranking prompt (Gemini picks and explains)
    answer = rerank_with_llm(results, normalized_query)

    # Optional: clarification if ambiguous
    ambiguous_phrases = [
        "Ù„Ø§ ØªÙˆØ¬Ø¯ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø¯", "Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©", "ØºÙŠØ± ÙƒØ§ÙÙŠØ©", "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø§Ø¯Ø©"
    ]
    if any(p in answer for p in ambiguous_phrases):
        answer += "ðŸ”Ž Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªÙˆØ¶ÙŠØ­ Ø³Ø¤Ø§Ù„Ùƒ Ø£ÙƒØ«Ø± Ø­ØªÙ‰ Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ø¯Ù‚Ø©."

    return {"answer": answer, "sources": results}

if __name__ == "__main__":
    # This main block is for local testing and might need genai configuration
    # For example: genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
    # Ensure the API key is available if running this directly.
    
    # Critical: Initialize corpus here for __main__ execution
    # This was missing and would cause 'corpus_ids' not defined error later.
    corpus_ids, corpus_texts, corpus_emb = load_embeddings()
    
    print("Running local test for semantic_search_only:")
    # Check if corpus_emb is valid before searching
    if hasattr(corpus_emb, 'nelement') and corpus_emb.nelement() > 0 :
        for r in semantic_search_only("Ø³Ø¨Ø¨ Ø§Ù„Ø¬Ø±ÙŠÙ…Ø© Ù„Ù„Ù…Ø¬Ù†Ù‰ Ø¹Ù„ÙŠÙ‡ Ù…Ø±Ø¶Ø§", top_k=3):
            print(r["doc_id"], r["score"], "", r["text"], "")
    else:
        print("Skipping semantic_search_only test as corpus_emb is empty or invalid.")
    
    # Example of testing paraphrasing and reranking (requires API key & model init)
    # user_q = "Ù…Ø§ Ù‡ÙŠ Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ø³Ø±Ù‚Ø© Ø¨Ø§Ù„Ø¥ÙƒØ±Ø§Ù‡ØŸ"
    # print(f"
#Testing paraphrasing for: {user_q}")
    # paraphrases = generate_paraphrased_questions(user_q, n=2)
    # print(paraphrases)
    #
    # print("
#Testing reranking (mocked results):")
    # mock_results_for_rerank = [
    #     {"article_number": "101", "text": "Ù†Øµ Ø§Ù„Ù…Ø§Ø¯Ø© 101 Ø¹Ù† Ø§Ù„Ø³Ø±Ù‚Ø©."},
    #     {"article_number": "202", "text": "Ù†Øµ Ø§Ù„Ù…Ø§Ø¯Ø© 202 Ø¹Ù† Ø§Ù„Ø¥ÙƒØ±Ø§Ù‡."}
    # ]
    # reranked_answer = rerank_with_llm(mock_results_for_rerank, user_q)
    # print(reranked_answer)
